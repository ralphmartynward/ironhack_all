{"cells":[{"cell_type":"code","source":["!pip install -U scipy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B9-I0Gghr3Pu","executionInfo":{"status":"ok","timestamp":1681907893042,"user_tz":-120,"elapsed":10100,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"af306632-49b7-4823-e395-312e5d754e98"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy) (1.22.4)\n"]}]},{"cell_type":"markdown","source":["# Confidence intervals"],"metadata":{"id":"AALWldggwu17"}},{"cell_type":"markdown","source":["When we work with a dataset, we need to keep in mind that in fact we're working with a **sample** of the real **population**. Therefore, any statistic (mean, sd,...etc) that we can compute from our **sample** it will not have the same value as the statistic of the real **population**. It will be a single **estimation** and it will be off by a centain unknown amount called **sampling error**. Furthermore, if we wait until we have more data or we simply pick another dataset from a different source of data and we compute the same statistic we will get a different value because our **sample** will be different. Thus, in general them mean of our sample will not be the same as the mean of the population:\n","\n","$$(sample\\_mean=\\bar{x}) \\neq (population\\_mean=\\mu)$$\n","\n","$$\\mu = \\bar{x} + \\delta(n)$$\n","\n","where $\\delta$ is the **sampling error** that will be different for each sample.This **sampling error** will depend on the **sample size** $n$. The bigger the sample, the smaller the error.\n","\n","Knowing this, could be a bit dissapointing as we don't have any way to determine how big is going to be the **sampling error**. To get a better idea of what could be the real **population mean** we could collect several samples and compute their corresponding **sample means**. Each sample mean will be off by an unknown amount and we might think that we are in the sample place where we started. However, we could make use of the **central limit theorem** and compute **the mean of several sample means** to get a more reliable **estimation**. But this strategy takes time and/or money and we might mot be able to implement it. \n","\n","Then, what else we can do? We can use the **central limit theorem** in our favour. What this theorem says is that the **sampling distribution** (the distribtuion followed by the means of our samples) follows a **normal distribution** regardeless of the original **population distribution**. In other words, if we could be able to collect many samples and to compute the mean of each of sample, when we will plot the distribution followed by these **sample means**, we will find a normal distribution centered on the **population mean**. Therefore, if we substract the population mean from our sample mean, the resulting value will be a random value that will follow a **normal distribution** centered on zero:\n","\n","$$\\bar{x}-\\mu = -\\delta(n) = ùí©(0,œÉ)$$\n","\n","if we divide both sides of the equation by the population's standard deviation we will have a normal distribution with $\\mu=0$ and $\\sigma=1$:\n","\n","$$z = \\frac{(\\bar{x}-\\mu)}{\\sigma}= -\\frac{\\delta(n)}{\\sigma} = ùí©(0,1)$$\n","\n","However, not allways we will know the population's standard deviation $\\sigma$ but we can have a reliable estimation using our sample:\n","\n","$$\\sigma ‚âà \\hat{\\sigma}=\\sqrt{ \\sum \\frac{(x - \\bar{x})^2}{n-1} }$$\n","\n","As we know the $ùí©(0,1)$ distribution we can use it to provide a range of values around our sample mean that very likely will contain the real **population value**. The idea is then to take one sample, compute our sample mean $\\bar{x}$, and then provide a range of values around our sample mean $\\bar{x}$ that very likely will contain the real population mean. How likely? As much as we want: 90%, 95%, 98%..."],"metadata":{"id":"dDuNfpw5w06R"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"ICBZQ4eFrEEu","executionInfo":{"status":"ok","timestamp":1681908129910,"user_tz":-120,"elapsed":190,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}}},"outputs":[],"source":["# 1: confidence interval\n","import numpy as np\n","\n","\n","# create some (fake) data of patient's cholesterol level,\n","# we'll assume this was obtained through measurements\n","# np.random.normal(mean, sd, number_of_samples)\n","np.random.seed(17)\n","\n","# Getting 100 values randomly selected from a normal distribution with mean=5.1 and sd=1.6\n","patients = np.random.normal(5.1, 1.6, 100)  \n","#     targeted mean value    ^\n","#  with a standard dev of         ^\n","# number of random values to generate  ^"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"EBx2Xju8rEE0","outputId":"50dba000-eeda-4f92-c95b-35e08dfd05d8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681908135050,"user_tz":-120,"elapsed":230,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 5.54202542,  2.13259507,  6.09824178,  6.93249806,  6.75950475,\n","        8.11862229,  4.92128274,  4.52063786,  5.33788007,  4.39954696,\n","        8.5740112 ,  6.9436964 ,  2.18990026,  4.87912105,  5.96374338,\n","        2.25954834,  7.20380246,  4.34248312,  3.35243216,  4.69995609,\n","        3.52832911,  6.75003055,  5.88613404,  4.38536543,  3.80982387,\n","        5.31002841,  3.15990362,  5.35598537,  3.89164314,  5.65983359,\n","        6.66406682,  4.8782636 ,  5.26617009,  5.58094566,  6.64912847,\n","        6.49139815,  6.00845294,  5.84445175,  3.23540307,  1.84240833,\n","        3.25133873, 10.45225183,  5.30276354,  3.98931377,  5.99227909,\n","        5.25863456,  6.12068188,  6.22497709,  3.63425095,  3.84237724,\n","        6.89069087,  3.52656623,  5.49123203,  4.16974442,  5.78730224,\n","        6.37744318,  4.12388792,  6.99664673,  3.96266559,  3.85019371,\n","        4.73140324,  5.29231213,  3.83844826,  0.36380381,  3.82707021,\n","        5.5553316 ,  5.93605965,  5.23368711,  9.45327294,  6.07761261,\n","        3.76248961,  5.09408633,  5.72895149,  6.19519213,  6.34032269,\n","        5.98960462,  4.23443216,  7.5390676 ,  7.03341621,  5.18204366,\n","        4.41739006,  6.39270436,  5.19164762,  3.43572548,  6.59463694,\n","        4.90794473,  8.63122609, -0.33988925,  7.07364623,  4.15939066,\n","        3.53690093,  5.84891248,  9.69380308,  3.16706291,  6.70576509,\n","        6.90916231,  5.99606351,  4.11056432,  4.74032205,  8.64544232])"]},"metadata":{},"execution_count":3}],"source":["patients"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Mm7PiAhOrEE1","outputId":"2a54e49a-9200-4c59-8b97-3adb855b7839","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681908152979,"user_tz":-120,"elapsed":259,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["5.278415670796427"]},"metadata":{},"execution_count":4}],"source":["# we can find out the sample mean, it is \n","patients.mean()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"NeaR-988rEE2","executionInfo":{"status":"ok","timestamp":1681908350348,"user_tz":-120,"elapsed":1040,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}}},"outputs":[],"source":["# we can find a confidence interval for the population based on our sample\n","import scipy.stats\n","\n","confidence_level = 0.95\n","degrees_freedom = len(patients) - 1  # or, for large values of sample size, just use that\n","sample_mean = np.mean(patients)\n","# note that we use the standard error of the sample \n","# is an estimate of the standard error of the population (which is used in the theoretical formula)\n","sample_standard_error = scipy.stats.sem(patients) # sem = standard error of the mean = std(mean)/sqrt(samplesize)\n","\n","confidence_interval = scipy.stats.t.interval(confidence_level, \n","                                             degrees_freedom, \n","                                             sample_mean, \n","                                             sample_standard_error)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"NsewLeBlrEE2","outputId":"2d87b5b9-427d-4018-8068-a54d31408819","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681908352893,"user_tz":-120,"elapsed":244,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["confidence interval is  (4.920803322187407, 5.6360280194054475) .\n"]}],"source":["print( 'confidence interval is ', confidence_interval, '.' )"]},{"cell_type":"code","source":["confidence_interval_normal = scipy.stats.norm.interval(confidence_level,  \n","                                             sample_mean, \n","                                             sample_standard_error)"],"metadata":{"id":"whQF5vftNSKF","executionInfo":{"status":"ok","timestamp":1681908474913,"user_tz":-120,"elapsed":227,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print( 'confidence interval is ', confidence_interval_normal, '.' )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFZysszyNaNt","executionInfo":{"status":"ok","timestamp":1681908477476,"user_tz":-120,"elapsed":5,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"571f4e65-4272-48d4-baf7-d1a1b3bd4c31"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["confidence interval is  (4.925174396868425, 5.631656944724429) .\n"]}]},{"cell_type":"markdown","source":["But what this mean? It means that if we would take a large number of samples of 100 patients, and we compute the confidence intervals around each sample mean,\n","about 95% of the confidence intervals will contain the real population mean  within the (lower,upper) values"],"metadata":{"id":"uT0Zmv7TcVXX"}},{"cell_type":"markdown","source":["# Hypothesis testing"],"metadata":{"id":"gOSsPJ7AT6IR"}},{"cell_type":"markdown","source":["Hypothesis tests can be bradly classified into two types:\n","\n","\n","*   One sided\n","*   Two sided\n","\n","The type of test, depends on the alternate Hypothesis. If the alternate hypothesis contains $H_{0/a}$ an $\\neq$, then is a **two sided test** as it contains **two rejection areas**. In any other case, it will be a **one sided test**.\n","\n","In addition, we can find two sub types of **one sided tests** depending on which side of the distribution is located the rejection area:\n","\n","* Left side\n","* Right side\n","\n","Again, the location of the rejection area depends on the alternate hypothesis $H_{0/a}$. To know where is located we need to look at the unequality. If the alternate hypothesis says $<$m then the rejection area is on the **left hand side of the distribution**. Otherwise in on the **right hand side**."],"metadata":{"id":"Kvo1xkS4UKV-"}},{"cell_type":"markdown","source":["What we do with a Hopytesis testing is get a sample and based on our sample try to reject the **null hypothesis**. To do this, we compute what is the probability to get a sample as **extreme or more** than the one we have obtained according to the distribution followed by the **null hypotesis**. In other words: when the **null hypothesis is True** what is the probability to get a sample like the one we have obtained? If this probability is **too small** we will **reject the null hypothesis** (even though that it can be possible to get it). Otherwise, we accept the **null hypothesis**. Then, the question is: \n","\n","What probability we consider a small probability? 0.05, 0.01, 0.001?\n","\n","This value is what we call our **significance level $\\alpha$**. The complementary of $\\alpha$, ie $1-\\alpha$ is called **confidence level** as is tipically given in %: 90%, 95%, 99%...etc. In other words, the **significance level** $\\alpha$ is probability threshold to consider a given probability too small or not. For example, if we set $\\alpha=0.05$, then any probability smaller than 0.05 will be considered too small and we will reject the **null hypothesis**. Otherwise, we will accept the **null hypothesis**."],"metadata":{"id":"7-N5BjSqUFmM"}},{"cell_type":"markdown","source":["## Two sided test example"],"metadata":{"id":"qP3f-9uEWBwV"}},{"cell_type":"markdown","source":["Let's assume that  someone thinks (hypothesis) that the mean \n","cholesterol level in the population is 5.6. Then, he/she takes blood samples from 100 individuals, and computes the sample mean. He/she want to make a hypothesis test. Let's see how to do it using Python. "],"metadata":{"id":"SVU01bbXWE-3"}},{"cell_type":"markdown","source":["In this case, the $H_{0}$ hypothesis is:\n","\n","* $H_{0}: \\mu = 5.6$\n","* $H_{1/a}: \\mu \\neq 5.6$\n","\n","As the alternate hypothesis contains an $\\neq$ we have two rejection areas. Why?\n","Because the sample mean could be different from the reference value because it's bigger or lower than the reference value. In general, we will never have a value equal to the population mean because of the **sampling error**. Every sample is random, therefore, the values in the sample will be random and the sample mean will be a random value. The question is: \n","\n","How big can be the discrepancy between our sample mean and the reference value in order to reject the $H_{0}$ hypothesis? In other words, how likely/unlikely is to get our sample mean given the $H_{0}$ hypothesis?"],"metadata":{"id":"1Q-4EV3rW3cs"}},{"cell_type":"markdown","source":["In this case, $\\alpha$ is the size of the **total rejection area**. However, we have **two** rejection areas equaly size. Therefore, the size of both have to add $\\alpha$. For this reason, the size of each rejection area is $\\alpha/2$."],"metadata":{"id":"gY246ZMTXOZJ"}},{"cell_type":"markdown","source":["To fix things, let's work with a **significance level $\\alpha=0.05$**."],"metadata":{"id":"2636T4r_amXi"}},{"cell_type":"markdown","source":["### Using Scipy"],"metadata":{"id":"MCWVsb6BcQBp"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"3_K1movxrEE4","executionInfo":{"status":"ok","timestamp":1681908787733,"user_tz":-120,"elapsed":206,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}}},"outputs":[],"source":["from scipy.stats import ttest_1samp\n","# someone thinks (hypothesises) that the mean of \n","# cholesterol values in the population is 5.6\n","\n","# we select a value for alpha of 0.05 (p-value threshold)\n","# Two-sided test:\n","# Null hypothesis or H0: mean cholesterol value = 5.6\n","# Alternative hyp or H1: mean cholesterol value != 5.6\n","stat, pval = ttest_1samp(patients, popmean = 5.6, alternative = \"two-sided\") \n"]},{"cell_type":"code","source":["stat, pval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_qVlOZfxQw_","executionInfo":{"status":"ok","timestamp":1681908790738,"user_tz":-120,"elapsed":227,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"ac340ab3-1f6d-4985-9126-6544f4b05fe5"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(-1.7843149987052263, 0.07743590566997763)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["The first value obtained before, is the value of the statistic while the second is the area that corresponds to the statistic. The other value $p_value$ is the total area (probability) associated with the statistic at both sides of the distribution. In other words:\n","\n","$$p_{val} = P(D|H_{0}=True)= scipy.stats.t.cdf(-1.78, ddof=n-1) +( 1 - scipy.stats.t.cdf(1.78, ddof=n-1)) $$\n","\n","where $n$ is our sample size (100 in our case). Remember that we have two rejection areas!!!\n","\n","In ourt case our p_value (0.077) is bigger than our significance level $\\alpha=0.05$. Therefore **we accept the null hypothesis**"],"metadata":{"id":"OmWkAxjsXyuF"}},{"cell_type":"markdown","source":["### Manual calculation using the statistic method."],"metadata":{"id":"x2Y7rVH6cscP"}},{"cell_type":"code","source":["# Computing manually our statistic.\n","# Note the use of ddof=1 in the np.std. We do it because we don't know the population\n","# standard deviation and we are estimating the value with a sample. \n","t = (np.mean(patients) - 5.6)/(np.std(patients, ddof=1)/np.sqrt(len(patients)))\n","print(\"The value of the statistic is: \",t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tVJYT-FscwNg","executionInfo":{"status":"ok","timestamp":1681908963620,"user_tz":-120,"elapsed":226,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"03b19f56-7f42-4578-9b42-d40b0fb4eecc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["The value of the statistic is:  -1.7843149987052263\n"]}]},{"cell_type":"markdown","source":["$\\sigma ‚âà \\sqrt{\\sum\\frac{1}{(n-1)}(x-\\bar{x})^{2}}$"],"metadata":{"id":"EdpIQ7oUl8mV"}},{"cell_type":"markdown","source":["We can see that the value that we obtain for the statistis is the same as the one computed earlier with the `ttest_1samp` function. Great!"],"metadata":{"id":"pXYYxFe8d4SE"}},{"cell_type":"markdown","source":["Now, let's compute what are the values on the x-axis that will correspond to $\\alpha/2=0.025$. Those values are called the **critical values**. As we have two rejection areas, we will have **two critical values**. Once we know them, we will be able to compare our statistic against the **critical values**. Let's see how to do it."],"metadata":{"id":"f1EdqArgbkqP"}},{"cell_type":"code","source":["lower_critical_value, upper_critical_value = scipy.stats.t.ppf((0.05/2), df=len(patients)-1), scipy.stats.t.ppf(1-(0.05/2), df=len(patients)-1)\n","print(\"The lower critical value is {:.3f}\".format(lower_critical_value))\n","print(\"The upper critical value is {:.3f}\".format(upper_critical_value))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dd-rcq2NcUJp","executionInfo":{"status":"ok","timestamp":1681909870933,"user_tz":-120,"elapsed":214,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"f01aee54-babf-462b-db0a-5dceb2e11e5b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["The lower critical value is -1.984\n","The upper critical value is 1.984\n"]}]},{"cell_type":"markdown","source":["Now, we compare our statistc againt these critical values. If our statistic is within thee two values, we will accept the **null hypothesis**. Otherwise we will reject it. In our case\n","\n","$$-1.984 < -1.784 < 1.984 $$\n","\n","Therefore, **we accept the null hypothesis**, which is the same conclussion that we got using the `ttest_1samp` function. Great!"],"metadata":{"id":"njY0St4IdcFu"}},{"cell_type":"markdown","source":["### Manual calculation using the p_value"],"metadata":{"id":"DwVzS4PDJ_K-"}},{"cell_type":"markdown","source":["When we use the p_value, what we need to do, is to compute what is the area that corresponds to our statistic -1.784. This is given by the **cumulative distribution function cdf**. Then, we will have to determine is this area is bigger than $\\alpha$ and smaller than $1-\\alpha$. If that's the case, we will **accept the null hypothesis**. Let's do it! "],"metadata":{"id":"us-0r7n5e3C8"}},{"cell_type":"code","source":["scipy.stats.t.cdf(t, df=len(patients)-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cLU4NevjdQq-","executionInfo":{"status":"ok","timestamp":1681910031529,"user_tz":-120,"elapsed":6,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"62decdda-5241-4d52-e0d7-56263c661b29"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.038717952834988814"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["In our case:\n","\n","$$ \\alpha/2 = 0.025 < 0.0387 < 1 - \\alpha/2 = 0.975 $$"],"metadata":{"id":"Zp-InVLzgSAb"}},{"cell_type":"markdown","source":["Again, we reach the same conclussion as with the other two methods: **we accept the null hypothesis**. Great!"],"metadata":{"id":"TwKPYEAmKIeO"}},{"cell_type":"markdown","source":["## One sided test"],"metadata":{"id":"rr_Uv-k5J3yd"}},{"cell_type":"markdown","source":["Now let's assume that our null hypothesis $H_{0}$  is that the cholesterol values are bigger than 5.6. Then in this case the hypothesis are:\n","\n","* $H_{0}: \\mu \\geq 5.6$\n","* $H_{1/a}: < 5.6$\n","\n","In this case, we have a one-sided test because we only have one rejection area. Why? Because the only way to have a discrepancy with the null $H_{0}$ hypothesis is to have a sample mean lower that the reference value. In other words, if our sample mean is greater than the reference value, our sample mean value will be consistent with the reference value.\n","\n","According to the distribution, having a value lower than the reference value is also possible. Then the question is:\n","\n","How likely is to have a sample mean value lower than the reference value according to the distribution? If this probability is too low (less than $\\alpha$), we will reject the null hypothesis $H_{0}$"],"metadata":{"id":"IWGze3W1ZD7K"}},{"cell_type":"markdown","source":["### Using Scipy"],"metadata":{"id":"R6LOxMAvhkWp"}},{"cell_type":"code","source":["# One-sided test:\n","# Null hypothesis or H0: mean cholesterol value >= 5.6\n","# Alternative hyp or H1: mean cholesterol value < 5.6\n","\n","\n","# ttest_1samp(sample, population_mean)\n","stat, pval = ttest_1samp(patients, popmean = 5.6, alternative = \"less\") "],"metadata":{"id":"k7tSSU5cY7KX","executionInfo":{"status":"ok","timestamp":1681910388122,"user_tz":-120,"elapsed":212,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["stat, pval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9J5QGWr_wQUl","executionInfo":{"status":"ok","timestamp":1681910391190,"user_tz":-120,"elapsed":221,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"f1fd0635-266f-4293-e346-0020f25d381e"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(-1.7843149987052263, 0.038717952834988814)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["Of course our statistic and the p_value is the same. What has changed are our **null and alternate hypothesis**. Now, we only have **one rejection area** located on the **left hand side of the distribution**. According to the output:\n"," \n","$$0.039 <\\alpha$$\n","\n","Therefore, **we reject the null hypothesis**."],"metadata":{"id":"HH7l82tahrFI"}},{"cell_type":"markdown","source":["###Manual calculation using the statistic method."],"metadata":{"id":"x0EwOjVVi46Y"}},{"cell_type":"code","source":["# Computing manually our statistic.\n","t = (np.mean(patients) - 5.6)/(np.std(patients, ddof=1)/np.sqrt(len(patients)))"],"metadata":{"id":"RjDBtw6kwWQD","executionInfo":{"status":"ok","timestamp":1681910580671,"user_tz":-120,"elapsed":270,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["Now, as we have only one rejection area, we will only have one **critical value**. Let's compute what is the value on the x-axis that corresponds to an area of $\\alpha$."],"metadata":{"id":"JEZw8Xqgi77O"}},{"cell_type":"code","source":["critical_value = scipy.stats.t.ppf(0.05, df=len(patients)-1)\n","print(\"The critical value is {:.3f}\".format(critical_value))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YeoKZuTQjPjC","executionInfo":{"status":"ok","timestamp":1681910646688,"user_tz":-120,"elapsed":227,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"03279750-be36-4b61-9182-ed87712ac543"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["The critical value is -1.660\n"]}]},{"cell_type":"markdown","source":["In our case, we have:\n","\n","$$ -1.784 < -1.660$$\n","\n","our statistics t is smaller than the critical value. Therefore, we **reject the null hypothesis** which is the same conclussion that we have obtained using the `ttest_1samp` function. Great!! "],"metadata":{"id":"dpPhn68SkOcV"}},{"cell_type":"markdown","source":["### Manual calculation using the p_value"],"metadata":{"id":"tALRwHqklAj7"}},{"cell_type":"markdown","source":["To use this method, we need to know which area corresponds to our statistic t and compare it with $alpha$. If the area that corresponds to our statistic is smaller than $\\alpha=0.05$ we will **reject the null hypothesis**. Othewise, we will accept it."],"metadata":{"id":"6ZsYMSD4lDkE"}},{"cell_type":"code","source":["scipy.stats.t.cdf(t, df=len(patients)-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDS9GG07w_qj","executionInfo":{"status":"ok","timestamp":1681910713322,"user_tz":-120,"elapsed":228,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"40885164-fc58-4093-f0f5-f492c292ab8b"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.038717952834988814"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["In our case:\n","\n","$$ 0.039 < \\alpha=0.05$$\n","\n","Therefore, we **reject the null hypothesis** which is the same conclussion that we got with the other two methods. Great!!"],"metadata":{"id":"mehC4PT7l3PS"}},{"cell_type":"markdown","source":["## One sided test (right)"],"metadata":{"id":"v_K4oaTAbbMG"}},{"cell_type":"markdown","source":["Now le'ts say that we believe that the cholesterol level is smaller than 5.6\n","\n","Then in this case the hypothesis are:\n","\n","* $H_{0}: \\mu \\leq 5.6$\n","* $H_{1/a}: \\mu > 5.6$\n","\n","In this case, we have a one-sided test because we only have one rejection area. Why? Because the only way to have a discrepancy with the null $H_{0}$ hypothesis is to have a sample mean bigger that the reference value. If our sample mean is lower than the reference value, our sample mean value will be consistent with the reference value.\n","\n","According to the distribtion, having a value bigger than the reference value is also possible. Then the question is:\n","\n","How likely is to have a sample mean value bigger than the reference value according to the distribution? If this probability is too low, we will reject the null hypothesis $H_{0}$"],"metadata":{"id":"3O47YSVabeR5"}},{"cell_type":"markdown","source":["### Using Scipy"],"metadata":{"id":"aybLSfVrmcyd"}},{"cell_type":"code","source":["# One-sided test:\n","# Null hypothesis or H0: mean cholesterol value <= 5.6\n","# Alternative hyp or H1: mean cholesterol value > 5.6\n","\n","\n","# ttest_1samp(sample, population_mean)\n","stat, pval = ttest_1samp(patients, popmean = 5.6, alternative = \"greater\") "],"metadata":{"id":"kQPifxu4bfk7","executionInfo":{"status":"ok","timestamp":1681910846002,"user_tz":-120,"elapsed":259,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["stat, pval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4HIqDmLmiV_","executionInfo":{"status":"ok","timestamp":1681910848426,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"c9d6534a-c918-4cd2-9e05-2a078142eadb"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(-1.7843149987052263, 0.9612820471650112)"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["Once again, our statistic is the same. However, our **null and alternate hypothesis are different**. \n","\n","In this case the p_value is bigger than $\\alpha=0.05$. Therefore we **accept the null hypothesis**"],"metadata":{"id":"Fi6unfr7nKpg"}},{"cell_type":"markdown","source":["### Manual calculation using the statistic method."],"metadata":{"id":"caB0gY8dnm0W"}},{"cell_type":"markdown","source":["Again, in this case we will only have one rejection area located on the right hand side of the distribution. The size of it it will be $\\alpha$. We need to compare our statistic against the **critical value**. If our statistic is smaller than the critical value, we will **accept the nul hypothesis**. Otherwise, we will **reject the null hypothesis**. Let's compute the **critical value**"],"metadata":{"id":"VHXSiAMknsda"}},{"cell_type":"code","source":["# Compute your p_value\n","critical_value = scipy.stats.t.ppf(1-0.05, df=len(patients)-1)\n","print(\"The critical value is {:.3f}\".format(critical_value))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxiLUUm32TXs","executionInfo":{"status":"ok","timestamp":1681911044219,"user_tz":-120,"elapsed":205,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"8ecc25e3-a738-475c-f299-25c5e302d2bf"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["The critical value is 1.660\n"]}]},{"cell_type":"markdown","source":["In our case, we have:\n","\n","$$-1.784 < 1.660$$\n","\n","Therefore, we **accept the null hypothesis** which is the same conclussion that we obtained with the previous method. Great!!"],"metadata":{"id":"L9OR6K1NpSaG"}},{"cell_type":"markdown","source":["### Manual calculation using the p_value"],"metadata":{"id":"m487kdSGpqjS"}},{"cell_type":"markdown","source":["Finally, in this case we need to know what is the area that corresponds to our statistic t and compare it against $1 - \\alpha$. If our area is smaller than $1 - \\alpha$ we **accept the null hypothesis**. \n","\n","Then, we need to know use the cdf."],"metadata":{"id":"gkYfWePTpzTm"}},{"cell_type":"code","source":["scipy.stats.t.cdf(t,df=len(patients)-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ZRyXkSw0_qu","executionInfo":{"status":"ok","timestamp":1681911191426,"user_tz":-120,"elapsed":332,"user":{"displayName":"Ignacio Soteras","userId":"02050793736257155229"}},"outputId":"04eb7dd2-b7c8-44d8-a86b-a14cd68863da"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.038717952834988814"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["In our case:\n","\n","$$0.038 < 1 - \\alpha = 0.95$$\n","\n","Therefore, we **accept the null hypothesis**. Great!!"],"metadata":{"id":"7Bu_wYUaqliU"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}